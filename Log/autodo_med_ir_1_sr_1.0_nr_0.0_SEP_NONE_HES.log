[2022-08-09 23:38:27,883] implicit-augment.py->main line:105 [INFO]Namespace(aug_model='SEP', data='./local_data', dataset='med', epochs=20, gpu='1', hyper_alpha=0.01, hyper_beta=0, hyper_est=True, hyper_gamma=0, hyper_iters=5, hyper_opt='HES', hyper_steps=0, imbalance_ratio=1, log_interval=500, los_model='NONE', lr_cosine=True, lr_decay_epochs='150,175,195', lr_decay_rate=0.1, lr_warm=True, lr_warm_epochs=5, no_cuda=False, noise_ratio=0.0, overfit=False, oversplit=False, plot_debug=False, run_folder='run0', scale=1, subsample_ratio=1.0, workers=4)
[2022-08-09 23:38:30,351] implicit-augment.py->main line:302 [INFO]Valid/Train Split: 2383/9532
[2022-08-09 23:38:30,352] implicit-augment.py->main line:373 [INFO]Test/Valid/Train Split: 2319/2383/9532 out of total 11915 train images
[2022-08-09 23:38:31,311] implicit-augment.py->main line:422 [INFO]Run: ./local_data/med/run0/UNet_opt_HES_est_True_aug_model_SEP_los_model_NONE_ir_1_sr_1.0_nr_0.0
[2022-08-09 23:38:31,311] implicit-augment.py->main line:429 [INFO]0% (0/20)
[2022-08-09 23:54:39,362] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 0	 Inner Train loss: 0.7682, acc=0.7110, lr=0.000010	
[2022-08-09 23:55:56,224] automodels.py->Med_innerTest line:856 [INFO]Epoch: 0	 Test loss: 1.1509, score: 0.6491
[2022-08-09 23:55:56,224] implicit-augment.py->main line:455 [INFO]SAVING trained model at epoch 0 with 0.6491 Dice score
[2022-08-09 23:55:56,634] implicit-augment.py->main line:429 [INFO]5% (1/20)
[2022-08-10 00:12:06,666] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 1	 Inner Train loss: 0.6692, acc=0.7522, lr=0.000010	
[2022-08-10 00:13:23,655] automodels.py->Med_innerTest line:856 [INFO]Epoch: 1	 Test loss: 1.1499, score: 0.6170
[2022-08-10 00:13:23,656] implicit-augment.py->main line:429 [INFO]10% (2/20)
[2022-08-10 00:29:32,937] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 2	 Inner Train loss: 0.6285, acc=0.7683, lr=0.000010	
[2022-08-10 00:30:50,817] automodels.py->Med_innerTest line:856 [INFO]Epoch: 2	 Test loss: 1.1937, score: 0.6522
[2022-08-10 00:30:50,817] implicit-augment.py->main line:455 [INFO]SAVING trained model at epoch 2 with 0.6522 Dice score
[2022-08-10 00:30:51,230] implicit-augment.py->main line:429 [INFO]15% (3/20)
[2022-08-10 00:47:01,290] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 3	 Inner Train loss: 0.6012, acc=0.7797, lr=0.000010	
[2022-08-10 00:48:18,253] automodels.py->Med_innerTest line:856 [INFO]Epoch: 3	 Test loss: 1.3891, score: 0.5883
[2022-08-10 00:48:18,253] implicit-augment.py->main line:429 [INFO]20% (4/20)
[2022-08-10 01:04:28,252] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 4	 Inner Train loss: 0.5871, acc=0.7854, lr=0.000010	
[2022-08-10 01:05:45,353] automodels.py->Med_innerTest line:856 [INFO]Epoch: 4	 Test loss: 1.1517, score: 0.6375
[2022-08-10 01:05:45,354] implicit-augment.py->main line:429 [INFO]25% (5/20)
[2022-08-10 01:21:56,019] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 5	 Inner Train loss: 0.5727, acc=0.7905, lr=0.000010	
[2022-08-10 01:23:13,173] automodels.py->Med_innerTest line:856 [INFO]Epoch: 5	 Test loss: 1.0457, score: 0.6459
[2022-08-10 01:23:13,174] implicit-augment.py->main line:429 [INFO]30% (6/20)
[2022-08-10 01:23:59,483] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.012537	gLtNorm 0.3009 (0.3009)	gLvNorm 0.1374 (0.1374)	mvpNorm 0.3472 (0.3472)

[2022-08-10 01:24:56,864] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.012853	gLtNorm 0.2472 (0.2586)	gLvNorm 0.5616 (0.2168)	mvpNorm 0.8186 (0.4476)

[2022-08-10 01:25:57,771] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.013170	gLtNorm 0.0477 (0.2477)	gLvNorm 0.0861 (0.2001)	mvpNorm 0.2098 (0.4184)

[2022-08-10 01:26:58,728] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.013486	gLtNorm 0.0236 (0.2305)	gLvNorm 0.0393 (0.2005)	mvpNorm 0.0807 (0.4040)

[2022-08-10 01:27:59,362] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.013802	gLtNorm 0.0190 (0.2306)	gLvNorm 0.0246 (0.2008)	mvpNorm 0.0773 (0.3981)

[2022-08-10 01:28:59,860] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.014118	gLtNorm 0.1539 (0.2289)	gLvNorm 0.0285 (0.1967)	mvpNorm 0.0700 (0.3953)

[2022-08-10 01:30:00,269] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.014435	gLtNorm 0.0548 (0.2255)	gLvNorm 0.0438 (0.1941)	mvpNorm 0.1209 (0.3941)

[2022-08-10 01:31:00,505] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.014751	gLtNorm 0.0319 (0.2219)	gLvNorm 0.0420 (0.1965)	mvpNorm 0.0120 (0.3879)

[2022-08-10 01:32:00,772] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.015067	gLtNorm 0.0809 (0.2140)	gLvNorm 0.0361 (0.1981)	mvpNorm 0.0206 (0.3807)

[2022-08-10 01:33:00,905] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.015384	gLtNorm 0.1735 (0.2164)	gLvNorm 0.0855 (0.1931)	mvpNorm 0.0345 (0.3773)

[2022-08-10 01:34:01,003] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.015700	gLtNorm 0.1914 (0.2128)	gLvNorm 0.1774 (0.1931)	mvpNorm 0.6119 (0.3684)

[2022-08-10 01:35:00,967] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.016016	gLtNorm 0.1502 (0.2181)	gLvNorm 0.1847 (0.1933)	mvpNorm 0.0793 (0.3748)

[2022-08-10 01:36:01,126] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.016332	gLtNorm 0.0595 (0.2226)	gLvNorm 0.0159 (0.1934)	mvpNorm 0.1022 (0.3821)

[2022-08-10 01:37:01,028] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.016649	gLtNorm 0.0170 (0.2196)	gLvNorm 0.6172 (0.1956)	mvpNorm 0.5556 (0.3827)

[2022-08-10 01:38:00,930] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.016965	gLtNorm 0.0625 (0.2195)	gLvNorm 0.0566 (0.1933)	mvpNorm 0.0058 (0.3790)

[2022-08-10 01:39:00,913] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.017281	gLtNorm 0.0270 (0.2166)	gLvNorm 0.0664 (0.1924)	mvpNorm 0.0177 (0.3756)

[2022-08-10 01:40:00,906] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.017598	gLtNorm 0.0714 (0.2167)	gLvNorm 0.6634 (0.1926)	mvpNorm 0.8278 (0.3748)

[2022-08-10 01:41:00,898] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.017914	gLtNorm 0.0414 (0.2195)	gLvNorm 0.0514 (0.1918)	mvpNorm 0.0988 (0.3742)

[2022-08-10 01:42:01,071] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.018230	gLtNorm 0.0532 (0.2167)	gLvNorm 0.0671 (0.1924)	mvpNorm 0.0039 (0.3718)

[2022-08-10 01:43:01,035] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.018546	gLtNorm 0.1368 (0.2157)	gLvNorm 0.0145 (0.1914)	mvpNorm 0.2108 (0.3706)

[2022-08-10 01:44:01,000] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.018863	gLtNorm 0.2295 (0.2192)	gLvNorm 0.1699 (0.1924)	mvpNorm 0.0132 (0.3757)

[2022-08-10 01:45:00,978] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.019179	gLtNorm 0.7193 (0.2205)	gLvNorm 0.0700 (0.1916)	mvpNorm 1.1478 (0.3768)

[2022-08-10 01:46:00,915] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.019495	gLtNorm 0.0807 (0.2182)	gLvNorm 0.1018 (0.1914)	mvpNorm 0.3563 (0.3741)

[2022-08-10 01:47:00,859] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.019811	gLtNorm 0.0893 (0.2170)	gLvNorm 0.0638 (0.1919)	mvpNorm 0.0032 (0.3735)

[2022-08-10 02:04:00,643] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 6	 Inner Train loss: 0.5620, acc=0.7946, lr=0.000010	
[2022-08-10 02:05:18,486] automodels.py->Med_innerTest line:856 [INFO]Epoch: 6	 Test loss: 1.1460, score: 0.6419
[2022-08-10 02:05:18,486] implicit-augment.py->main line:429 [INFO]35% (7/20)
[2022-08-10 02:05:19,043] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.020074	gLtNorm 0.0626 (0.0626)	gLvNorm 0.3930 (0.3930)	mvpNorm 0.7065 (0.7065)

[2022-08-10 02:06:19,544] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.020390	gLtNorm 0.1415 (0.2087)	gLvNorm 0.1078 (0.1669)	mvpNorm 0.1155 (0.3627)

[2022-08-10 02:07:19,596] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.020707	gLtNorm 0.0479 (0.2025)	gLvNorm 1.3234 (0.1557)	mvpNorm 1.4010 (0.3271)

[2022-08-10 02:08:19,701] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.021023	gLtNorm 0.0114 (0.1900)	gLvNorm 0.3220 (0.1540)	mvpNorm 0.2188 (0.3128)

[2022-08-10 02:09:19,759] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.021339	gLtNorm 0.4534 (0.1802)	gLvNorm 0.0638 (0.1550)	mvpNorm 0.7012 (0.3083)

[2022-08-10 02:10:19,738] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.021655	gLtNorm 0.0117 (0.1865)	gLvNorm 0.0341 (0.1598)	mvpNorm 0.0240 (0.3193)

[2022-08-10 02:11:19,876] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.021972	gLtNorm 0.0421 (0.1877)	gLvNorm 0.1860 (0.1635)	mvpNorm 0.0632 (0.3243)

[2022-08-10 02:12:19,843] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.022288	gLtNorm 0.1300 (0.1893)	gLvNorm 0.8801 (0.1615)	mvpNorm 0.7376 (0.3253)

[2022-08-10 02:13:19,786] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.022604	gLtNorm 0.3419 (0.1854)	gLvNorm 0.1432 (0.1601)	mvpNorm 0.8575 (0.3202)

[2022-08-10 02:14:19,738] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.022921	gLtNorm 0.0079 (0.1812)	gLvNorm 0.0255 (0.1580)	mvpNorm 0.0504 (0.3137)

[2022-08-10 02:15:19,600] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.023237	gLtNorm 0.1361 (0.1813)	gLvNorm 0.1031 (0.1588)	mvpNorm 0.0452 (0.3175)

[2022-08-10 02:16:19,440] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.023553	gLtNorm 0.1135 (0.1829)	gLvNorm 0.0766 (0.1621)	mvpNorm 0.2997 (0.3230)

[2022-08-10 02:17:19,449] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.023869	gLtNorm 0.0079 (0.1867)	gLvNorm 2.4502 (0.1637)	mvpNorm 2.5673 (0.3293)

[2022-08-10 02:18:19,394] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.024186	gLtNorm 1.1640 (0.1850)	gLvNorm 0.0225 (0.1639)	mvpNorm 1.3631 (0.3282)

[2022-08-10 02:19:19,399] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.024502	gLtNorm 0.4665 (0.1851)	gLvNorm 0.0527 (0.1628)	mvpNorm 0.6365 (0.3244)

[2022-08-10 02:20:19,366] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.024818	gLtNorm 0.1186 (0.1849)	gLvNorm 0.9037 (0.1634)	mvpNorm 1.1042 (0.3250)

[2022-08-10 02:21:19,298] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.025135	gLtNorm 0.0638 (0.1854)	gLvNorm 0.0088 (0.1628)	mvpNorm 0.0470 (0.3268)

[2022-08-10 02:22:19,304] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.025451	gLtNorm 0.0565 (0.1869)	gLvNorm 0.1115 (0.1611)	mvpNorm 0.2556 (0.3259)

[2022-08-10 02:23:19,440] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.025767	gLtNorm 0.0525 (0.1854)	gLvNorm 0.0917 (0.1614)	mvpNorm 0.2301 (0.3243)

[2022-08-10 02:24:19,386] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.026083	gLtNorm 0.0546 (0.1846)	gLvNorm 0.0071 (0.1630)	mvpNorm 0.0535 (0.3241)

[2022-08-10 02:25:19,275] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.026400	gLtNorm 0.3040 (0.1830)	gLvNorm 0.1586 (0.1637)	mvpNorm 0.1268 (0.3240)

[2022-08-10 02:26:19,061] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.026716	gLtNorm 0.0659 (0.1861)	gLvNorm 0.0301 (0.1643)	mvpNorm 0.0227 (0.3284)

[2022-08-10 02:27:18,951] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.027032	gLtNorm 0.2809 (0.1841)	gLvNorm 0.1189 (0.1643)	mvpNorm 0.6565 (0.3278)

[2022-08-10 02:28:18,840] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.027348	gLtNorm 0.0607 (0.1853)	gLvNorm 0.0620 (0.1621)	mvpNorm 0.1986 (0.3267)

[2022-08-10 02:45:19,952] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 7	 Inner Train loss: 0.5545, acc=0.7975, lr=0.000010	
[2022-08-10 02:46:37,000] automodels.py->Med_innerTest line:856 [INFO]Epoch: 7	 Test loss: 1.1416, score: 0.6450
[2022-08-10 02:46:37,000] implicit-augment.py->main line:429 [INFO]40% (8/20)
[2022-08-10 02:46:37,560] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.027611	gLtNorm 0.1142 (0.1142)	gLvNorm 1.0841 (1.0841)	mvpNorm 1.8085 (1.8085)

[2022-08-10 02:47:38,182] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.027927	gLtNorm 0.0557 (0.2898)	gLvNorm 0.0508 (0.2316)	mvpNorm 0.0532 (0.4385)

[2022-08-10 02:48:38,435] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.028244	gLtNorm 0.0099 (0.2716)	gLvNorm 0.0396 (0.2410)	mvpNorm 0.0128 (0.4563)

[2022-08-10 02:49:38,612] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.028560	gLtNorm 0.0322 (0.2533)	gLvNorm 0.0313 (0.2224)	mvpNorm 0.0631 (0.4313)

[2022-08-10 02:50:38,694] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.028876	gLtNorm 2.4212 (0.2635)	gLvNorm 0.0376 (0.2170)	mvpNorm 1.8956 (0.4419)

[2022-08-10 02:51:38,730] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.029192	gLtNorm 0.2209 (0.2646)	gLvNorm 0.3352 (0.2160)	mvpNorm 0.5653 (0.4512)

[2022-08-10 02:52:38,987] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.029509	gLtNorm 0.0409 (0.2540)	gLvNorm 0.3525 (0.2143)	mvpNorm 0.1778 (0.4337)

[2022-08-10 02:53:39,035] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.029825	gLtNorm 0.1073 (0.2477)	gLvNorm 0.6729 (0.2095)	mvpNorm 0.8434 (0.4205)

[2022-08-10 02:54:39,035] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.030141	gLtNorm 0.0913 (0.2471)	gLvNorm 0.1555 (0.2092)	mvpNorm 0.0252 (0.4171)

[2022-08-10 02:55:39,012] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.030458	gLtNorm 0.0807 (0.2406)	gLvNorm 0.2223 (0.2102)	mvpNorm 0.1661 (0.4111)

[2022-08-10 02:56:38,965] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.030774	gLtNorm 0.0523 (0.2428)	gLvNorm 4.1073 (0.2152)	mvpNorm 4.7323 (0.4198)

[2022-08-10 02:57:38,942] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.031090	gLtNorm 0.6832 (0.2459)	gLvNorm 0.0430 (0.2163)	mvpNorm 0.6772 (0.4253)

[2022-08-10 02:58:39,060] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.031406	gLtNorm 0.1365 (0.2439)	gLvNorm 0.0789 (0.2153)	mvpNorm 0.1124 (0.4210)

[2022-08-10 02:59:39,029] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.031723	gLtNorm 0.0640 (0.2437)	gLvNorm 0.0488 (0.2144)	mvpNorm 0.0272 (0.4186)

[2022-08-10 03:00:38,983] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.032039	gLtNorm 0.1520 (0.2417)	gLvNorm 0.1251 (0.2167)	mvpNorm 0.1195 (0.4163)

[2022-08-10 03:01:38,992] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.032355	gLtNorm 0.3181 (0.2469)	gLvNorm 0.3663 (0.2189)	mvpNorm 0.6095 (0.4250)

[2022-08-10 03:02:38,960] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.032672	gLtNorm 0.0256 (0.2435)	gLvNorm 0.1082 (0.2184)	mvpNorm 0.0745 (0.4186)

[2022-08-10 03:03:38,912] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.032988	gLtNorm 0.0551 (0.2429)	gLvNorm 0.0587 (0.2168)	mvpNorm 0.0006 (0.4152)

[2022-08-10 03:04:39,018] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.033304	gLtNorm 0.6944 (0.2435)	gLvNorm 0.2820 (0.2187)	mvpNorm 1.4677 (0.4205)

[2022-08-10 03:05:38,942] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.033620	gLtNorm 0.0349 (0.2422)	gLvNorm 0.6846 (0.2161)	mvpNorm 0.7687 (0.4169)

[2022-08-10 03:06:38,921] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.033937	gLtNorm 0.6732 (0.2425)	gLvNorm 0.3190 (0.2185)	mvpNorm 1.2121 (0.4209)

[2022-08-10 03:07:38,847] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.034253	gLtNorm 0.0813 (0.2424)	gLvNorm 0.1602 (0.2173)	mvpNorm 0.0925 (0.4179)

[2022-08-10 03:08:38,776] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.034569	gLtNorm 0.0517 (0.2433)	gLvNorm 0.0398 (0.2180)	mvpNorm 0.1493 (0.4197)

[2022-08-10 03:09:38,661] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.034885	gLtNorm 0.2673 (0.2445)	gLvNorm 0.3565 (0.2163)	mvpNorm 1.1479 (0.4196)

[2022-08-10 03:26:39,134] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 8	 Inner Train loss: 0.5487, acc=0.7993, lr=0.000010	
[2022-08-10 03:27:56,847] automodels.py->Med_innerTest line:856 [INFO]Epoch: 8	 Test loss: 1.1426, score: 0.6599
[2022-08-10 03:27:56,847] implicit-augment.py->main line:455 [INFO]SAVING trained model at epoch 8 with 0.6599 Dice score
[2022-08-10 03:27:57,284] implicit-augment.py->main line:429 [INFO]45% (9/20)
[2022-08-10 03:27:57,839] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.035148	gLtNorm 0.0157 (0.0157)	gLvNorm 0.0020 (0.0020)	mvpNorm 0.0151 (0.0151)

[2022-08-10 03:28:58,288] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.035464	gLtNorm 0.0594 (0.2001)	gLvNorm 0.2250 (0.1811)	mvpNorm 0.5006 (0.3760)

[2022-08-10 03:29:58,451] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.035781	gLtNorm 0.5059 (0.1793)	gLvNorm 0.0444 (0.1600)	mvpNorm 0.7862 (0.3165)

[2022-08-10 03:30:58,554] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.036097	gLtNorm 0.0689 (0.1624)	gLvNorm 0.1312 (0.1665)	mvpNorm 0.0168 (0.3069)

[2022-08-10 03:31:58,640] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.036413	gLtNorm 0.1062 (0.1621)	gLvNorm 0.1809 (0.1680)	mvpNorm 0.1143 (0.2971)

[2022-08-10 03:32:58,630] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.036729	gLtNorm 0.0891 (0.1717)	gLvNorm 0.0080 (0.1656)	mvpNorm 0.1150 (0.3073)

[2022-08-10 03:33:58,728] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.037046	gLtNorm 0.1134 (0.1746)	gLvNorm 0.1993 (0.1689)	mvpNorm 0.0306 (0.3164)

[2022-08-10 03:34:58,699] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.037362	gLtNorm 0.2243 (0.1720)	gLvNorm 0.0452 (0.1700)	mvpNorm 0.1486 (0.3095)

[2022-08-10 03:35:58,637] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.037678	gLtNorm 0.0527 (0.1730)	gLvNorm 0.1290 (0.1707)	mvpNorm 0.0179 (0.3124)

[2022-08-10 03:36:58,537] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.037995	gLtNorm 0.0437 (0.1701)	gLvNorm 0.0617 (0.1692)	mvpNorm 0.0171 (0.3083)

[2022-08-10 03:37:58,443] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.038311	gLtNorm 0.1033 (0.1731)	gLvNorm 0.0341 (0.1701)	mvpNorm 0.0241 (0.3108)

[2022-08-10 03:38:58,363] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.038627	gLtNorm 0.3323 (0.1733)	gLvNorm 0.0550 (0.1687)	mvpNorm 0.1205 (0.3065)

[2022-08-10 03:39:58,461] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.038943	gLtNorm 0.0056 (0.1716)	gLvNorm 0.0105 (0.1674)	mvpNorm 0.0095 (0.3051)

[2022-08-10 03:40:58,424] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.039260	gLtNorm 0.0299 (0.1717)	gLvNorm 0.2276 (0.1668)	mvpNorm 0.3711 (0.3057)

[2022-08-10 03:41:58,419] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.039576	gLtNorm 0.0311 (0.1706)	gLvNorm 0.0693 (0.1670)	mvpNorm 0.0679 (0.3043)

[2022-08-10 03:42:58,408] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.039892	gLtNorm 0.0720 (0.1682)	gLvNorm 0.6962 (0.1681)	mvpNorm 0.8453 (0.3031)

[2022-08-10 03:43:58,383] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.040209	gLtNorm 0.0631 (0.1694)	gLvNorm 0.0298 (0.1672)	mvpNorm 0.0356 (0.3043)

[2022-08-10 03:44:58,306] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.040525	gLtNorm 0.2832 (0.1717)	gLvNorm 0.1334 (0.1688)	mvpNorm 0.0626 (0.3095)

[2022-08-10 03:45:58,301] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.040841	gLtNorm 0.0631 (0.1700)	gLvNorm 0.1290 (0.1682)	mvpNorm 0.0274 (0.3080)

[2022-08-10 03:46:58,186] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.041157	gLtNorm 0.2997 (0.1694)	gLvNorm 0.0412 (0.1678)	mvpNorm 0.5328 (0.3087)

[2022-08-10 03:47:58,054] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.041474	gLtNorm 0.1137 (0.1701)	gLvNorm 0.0232 (0.1672)	mvpNorm 0.0813 (0.3079)

[2022-08-10 03:48:57,910] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.041790	gLtNorm 0.0403 (0.1701)	gLvNorm 0.2314 (0.1681)	mvpNorm 0.2817 (0.3103)

[2022-08-10 03:49:57,832] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.042106	gLtNorm 0.5408 (0.1691)	gLvNorm 0.0207 (0.1689)	mvpNorm 0.5906 (0.3093)

[2022-08-10 03:50:57,813] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.042422	gLtNorm 0.5983 (0.1697)	gLvNorm 0.0640 (0.1681)	mvpNorm 0.9175 (0.3088)

[2022-08-10 04:07:58,188] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 9	 Inner Train loss: 0.5410, acc=0.8027, lr=0.000010	
[2022-08-10 04:09:15,951] automodels.py->Med_innerTest line:856 [INFO]Epoch: 9	 Test loss: 1.2658, score: 0.6228
[2022-08-10 04:09:15,951] implicit-augment.py->main line:429 [INFO]50% (10/20)
[2022-08-10 04:09:16,493] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.1089 (0.1089)	gLvNorm 1.7012 (1.7012)	mvpNorm 2.4578 (2.4578)

[2022-08-10 04:10:17,293] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.4215 (0.3543)	gLvNorm 0.0357 (0.2590)	mvpNorm 0.2389 (0.6327)

[2022-08-10 04:11:17,588] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0111 (0.2882)	gLvNorm 0.0284 (0.2450)	mvpNorm 0.0250 (0.5328)

[2022-08-10 04:12:17,727] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0451 (0.2858)	gLvNorm 0.1358 (0.2398)	mvpNorm 0.0292 (0.5152)

[2022-08-10 04:13:17,806] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0400 (0.2843)	gLvNorm 0.1447 (0.2466)	mvpNorm 0.2390 (0.5121)

[2022-08-10 04:14:17,810] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0685 (0.2772)	gLvNorm 0.1426 (0.2720)	mvpNorm 0.1692 (0.5291)

[2022-08-10 04:15:17,951] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0580 (0.3018)	gLvNorm 0.1367 (0.2619)	mvpNorm 0.3417 (0.5369)

[2022-08-10 04:16:17,918] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.1994 (0.3137)	gLvNorm 0.0920 (0.2696)	mvpNorm 0.0997 (0.5472)

[2022-08-10 04:17:17,883] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.3076 (0.3146)	gLvNorm 0.1313 (0.2637)	mvpNorm 0.0950 (0.5464)

[2022-08-10 04:18:17,841] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.1293 (0.3174)	gLvNorm 0.3555 (0.2582)	mvpNorm 0.7698 (0.5452)

[2022-08-10 04:19:17,910] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0445 (0.3024)	gLvNorm 1.0310 (0.2584)	mvpNorm 1.1461 (0.5332)

[2022-08-10 04:20:17,969] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0883 (0.3003)	gLvNorm 0.1089 (0.2579)	mvpNorm 0.0482 (0.5263)

[2022-08-10 04:21:18,234] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.2921 (0.2968)	gLvNorm 0.0854 (0.2611)	mvpNorm 0.6782 (0.5297)

[2022-08-10 04:22:18,359] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.1644 (0.2967)	gLvNorm 0.0173 (0.2578)	mvpNorm 0.0923 (0.5235)

[2022-08-10 04:23:18,403] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.1583 (0.2941)	gLvNorm 1.8928 (0.2585)	mvpNorm 1.4346 (0.5225)

[2022-08-10 04:24:18,336] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0178 (0.2979)	gLvNorm 0.4175 (0.2609)	mvpNorm 0.3610 (0.5321)

[2022-08-10 04:25:18,222] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0456 (0.2985)	gLvNorm 0.2950 (0.2589)	mvpNorm 0.2179 (0.5332)

[2022-08-10 04:26:18,176] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0071 (0.2991)	gLvNorm 0.0454 (0.2590)	mvpNorm 0.0692 (0.5346)

[2022-08-10 04:27:18,286] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0638 (0.3128)	gLvNorm 0.0429 (0.2615)	mvpNorm 0.0114 (0.5528)

[2022-08-10 04:28:18,306] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0325 (0.3080)	gLvNorm 0.2822 (0.2593)	mvpNorm 0.4760 (0.5458)

[2022-08-10 04:29:18,363] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0322 (0.3055)	gLvNorm 0.0551 (0.2578)	mvpNorm 0.0877 (0.5418)

[2022-08-10 04:30:18,398] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0882 (0.3049)	gLvNorm 0.1076 (0.2550)	mvpNorm 0.0101 (0.5384)

[2022-08-10 04:31:18,358] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.0228 (0.2979)	gLvNorm 0.0603 (0.2591)	mvpNorm 0.0461 (0.5360)

[2022-08-10 04:32:18,412] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.037513	gLtNorm 0.1002 (0.2981)	gLvNorm 0.0114 (0.2615)	mvpNorm 0.0969 (0.5385)

[2022-08-10 04:49:18,662] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 10	 Inner Train loss: 0.5363, acc=0.8045, lr=0.000010	
[2022-08-10 04:50:35,618] automodels.py->Med_innerTest line:856 [INFO]Epoch: 10	 Test loss: 1.1275, score: 0.6544
[2022-08-10 04:50:35,619] implicit-augment.py->main line:429 [INFO]55% (11/20)
[2022-08-10 04:50:36,164] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0237 (0.0237)	gLvNorm 0.0187 (0.0187)	mvpNorm 0.0048 (0.0048)

[2022-08-10 04:51:35,786] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.1500 (0.4057)	gLvNorm 0.0269 (0.1959)	mvpNorm 0.2327 (0.5635)

[2022-08-10 04:52:36,086] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.9115 (0.3275)	gLvNorm 0.0770 (0.2378)	mvpNorm 0.5067 (0.5089)

[2022-08-10 04:53:36,276] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0473 (0.3093)	gLvNorm 0.0168 (0.2502)	mvpNorm 0.0537 (0.4812)

[2022-08-10 04:54:36,292] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0718 (0.3189)	gLvNorm 0.0877 (0.2481)	mvpNorm 0.0054 (0.5118)

[2022-08-10 04:55:36,338] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.6535 (0.3031)	gLvNorm 0.0647 (0.2378)	mvpNorm 0.5628 (0.4966)

[2022-08-10 04:56:36,622] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0519 (0.2923)	gLvNorm 0.2637 (0.2381)	mvpNorm 0.2141 (0.4925)

[2022-08-10 04:57:36,678] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0877 (0.2850)	gLvNorm 0.6365 (0.2346)	mvpNorm 0.7175 (0.4849)

[2022-08-10 04:58:36,805] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0242 (0.2860)	gLvNorm 0.0218 (0.2313)	mvpNorm 0.0023 (0.4797)

[2022-08-10 04:59:36,952] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0162 (0.2822)	gLvNorm 0.0497 (0.2307)	mvpNorm 0.0146 (0.4817)

[2022-08-10 05:00:36,985] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.2677 (0.2769)	gLvNorm 0.1994 (0.2341)	mvpNorm 0.0514 (0.4798)

[2022-08-10 05:01:36,977] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.1498 (0.2711)	gLvNorm 0.0322 (0.2353)	mvpNorm 0.1310 (0.4752)

[2022-08-10 05:02:37,086] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0340 (0.2706)	gLvNorm 0.0327 (0.2367)	mvpNorm 0.0051 (0.4774)

[2022-08-10 05:03:37,041] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0253 (0.2766)	gLvNorm 0.1331 (0.2351)	mvpNorm 0.1660 (0.4797)

[2022-08-10 05:04:37,050] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.1151 (0.2758)	gLvNorm 0.0242 (0.2359)	mvpNorm 0.1274 (0.4797)

[2022-08-10 05:05:37,033] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0432 (0.2723)	gLvNorm 0.0308 (0.2361)	mvpNorm 0.0754 (0.4744)

[2022-08-10 05:06:37,078] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0930 (0.2776)	gLvNorm 1.3416 (0.2409)	mvpNorm 2.1363 (0.4834)

[2022-08-10 05:07:37,073] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.1949 (0.2755)	gLvNorm 0.0463 (0.2368)	mvpNorm 0.1743 (0.4751)

[2022-08-10 05:08:37,281] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.8709 (0.2719)	gLvNorm 0.0366 (0.2377)	mvpNorm 1.1623 (0.4709)

[2022-08-10 05:09:37,338] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0313 (0.2693)	gLvNorm 0.0232 (0.2360)	mvpNorm 0.0016 (0.4653)

[2022-08-10 05:10:37,313] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.2133 (0.2783)	gLvNorm 0.0384 (0.2397)	mvpNorm 0.1186 (0.4773)

[2022-08-10 05:11:37,231] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0551 (0.2757)	gLvNorm 0.1320 (0.2400)	mvpNorm 0.0502 (0.4752)

[2022-08-10 05:12:37,133] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.3516 (0.2747)	gLvNorm 0.0427 (0.2407)	mvpNorm 0.4271 (0.4725)

[2022-08-10 05:13:37,082] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.032743	gLtNorm 0.0704 (0.2732)	gLvNorm 0.1682 (0.2407)	mvpNorm 0.4302 (0.4722)

[2022-08-10 05:30:37,305] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 11	 Inner Train loss: 0.5325, acc=0.8065, lr=0.000010	
[2022-08-10 05:31:54,370] automodels.py->Med_innerTest line:856 [INFO]Epoch: 11	 Test loss: 1.1297, score: 0.6329
[2022-08-10 05:31:54,371] implicit-augment.py->main line:429 [INFO]60% (12/20)
[2022-08-10 05:31:54,933] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0722 (0.0722)	gLvNorm 0.0123 (0.0123)	mvpNorm 0.0427 (0.0427)

[2022-08-10 05:32:55,067] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0168 (0.1919)	gLvNorm 0.0520 (0.1687)	mvpNorm 0.0298 (0.3611)

[2022-08-10 05:33:55,430] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0718 (0.2028)	gLvNorm 0.0904 (0.1809)	mvpNorm 0.0066 (0.3984)

[2022-08-10 05:34:55,634] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0265 (0.2009)	gLvNorm 0.1015 (0.1618)	mvpNorm 0.0381 (0.3645)

[2022-08-10 05:35:55,739] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.8634 (0.1912)	gLvNorm 0.0768 (0.1585)	mvpNorm 1.2462 (0.3475)

[2022-08-10 05:36:55,910] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.1685 (0.1851)	gLvNorm 0.0667 (0.1590)	mvpNorm 0.2131 (0.3405)

[2022-08-10 05:37:56,280] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0748 (0.1765)	gLvNorm 0.0090 (0.1606)	mvpNorm 0.0779 (0.3320)

[2022-08-10 05:38:56,438] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.2429 (0.1735)	gLvNorm 0.0226 (0.1631)	mvpNorm 0.1219 (0.3318)

[2022-08-10 05:39:56,498] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0493 (0.1720)	gLvNorm 0.0483 (0.1612)	mvpNorm 0.0968 (0.3292)

[2022-08-10 05:40:56,431] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0879 (0.1797)	gLvNorm 0.1989 (0.1609)	mvpNorm 0.5382 (0.3379)

[2022-08-10 05:41:56,392] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.5072 (0.1800)	gLvNorm 0.0580 (0.1614)	mvpNorm 0.8783 (0.3358)

[2022-08-10 05:42:56,375] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.1455 (0.1772)	gLvNorm 0.1415 (0.1603)	mvpNorm 0.1044 (0.3319)

[2022-08-10 05:43:56,511] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0391 (0.1739)	gLvNorm 0.0395 (0.1596)	mvpNorm 0.0134 (0.3298)

[2022-08-10 05:44:56,396] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.2574 (0.1832)	gLvNorm 0.0892 (0.1625)	mvpNorm 0.1061 (0.3404)

[2022-08-10 05:45:56,383] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0604 (0.1858)	gLvNorm 0.2478 (0.1643)	mvpNorm 0.5303 (0.3429)

[2022-08-10 05:46:56,353] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0228 (0.1844)	gLvNorm 0.1531 (0.1646)	mvpNorm 0.2476 (0.3419)

[2022-08-10 05:47:56,296] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0137 (0.1837)	gLvNorm 0.0461 (0.1630)	mvpNorm 0.0277 (0.3389)

[2022-08-10 05:48:56,345] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.1125 (0.1861)	gLvNorm 0.0584 (0.1608)	mvpNorm 0.0189 (0.3388)

[2022-08-10 05:49:56,362] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.1507 (0.1854)	gLvNorm 0.0288 (0.1605)	mvpNorm 0.0612 (0.3368)

[2022-08-10 05:50:56,249] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.1020 (0.1850)	gLvNorm 0.5345 (0.1612)	mvpNorm 0.5580 (0.3361)

[2022-08-10 05:51:56,148] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0565 (0.1841)	gLvNorm 0.0178 (0.1626)	mvpNorm 0.0829 (0.3371)

[2022-08-10 05:52:56,128] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0112 (0.1843)	gLvNorm 0.0963 (0.1625)	mvpNorm 0.0927 (0.3361)

[2022-08-10 05:53:56,036] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0555 (0.1844)	gLvNorm 0.0054 (0.1614)	mvpNorm 0.0852 (0.3344)

[2022-08-10 05:54:55,958] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.027636	gLtNorm 0.0337 (0.1850)	gLvNorm 0.0516 (0.1614)	mvpNorm 0.0629 (0.3360)

[2022-08-10 06:11:55,030] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 12	 Inner Train loss: 0.5251, acc=0.8082, lr=0.000010	
[2022-08-10 06:13:11,376] automodels.py->Med_innerTest line:856 [INFO]Epoch: 12	 Test loss: 1.1780, score: 0.6411
[2022-08-10 06:13:11,377] implicit-augment.py->main line:429 [INFO]65% (13/20)
[2022-08-10 06:13:11,912] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0438 (0.0438)	gLvNorm 0.0438 (0.0438)	mvpNorm 0.0348 (0.0348)

[2022-08-10 06:14:11,159] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0901 (0.1421)	gLvNorm 0.0362 (0.1620)	mvpNorm 0.0979 (0.3011)

[2022-08-10 06:15:10,146] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.2616 (0.1386)	gLvNorm 0.1419 (0.1349)	mvpNorm 0.7465 (0.2743)

[2022-08-10 06:16:09,034] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0546 (0.1394)	gLvNorm 0.0583 (0.1409)	mvpNorm 0.0376 (0.2799)

[2022-08-10 06:17:07,854] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0070 (0.1337)	gLvNorm 0.0327 (0.1405)	mvpNorm 0.0337 (0.2727)

[2022-08-10 06:18:06,576] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 5.6798 (0.1364)	gLvNorm 0.3304 (0.1423)	mvpNorm 8.3439 (0.2792)

[2022-08-10 06:19:05,492] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0300 (0.1389)	gLvNorm 0.2701 (0.1400)	mvpNorm 0.1560 (0.2768)

[2022-08-10 06:20:04,288] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0220 (0.1373)	gLvNorm 0.3322 (0.1395)	mvpNorm 0.3301 (0.2736)

[2022-08-10 06:21:03,090] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.1271 (0.1446)	gLvNorm 0.0202 (0.1404)	mvpNorm 0.2407 (0.2835)

[2022-08-10 06:22:01,844] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0245 (0.1496)	gLvNorm 0.0140 (0.1397)	mvpNorm 0.0185 (0.2881)

[2022-08-10 06:23:00,617] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0794 (0.1525)	gLvNorm 0.1515 (0.1415)	mvpNorm 0.4437 (0.2911)

[2022-08-10 06:23:59,273] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.2235 (0.1563)	gLvNorm 0.0125 (0.1386)	mvpNorm 0.3206 (0.2932)

[2022-08-10 06:24:58,228] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0520 (0.1555)	gLvNorm 0.0148 (0.1389)	mvpNorm 0.0358 (0.2937)

[2022-08-10 06:25:56,918] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0687 (0.1563)	gLvNorm 0.1341 (0.1379)	mvpNorm 0.1833 (0.2944)

[2022-08-10 06:26:55,599] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0476 (0.1561)	gLvNorm 0.0206 (0.1369)	mvpNorm 0.0313 (0.2933)

[2022-08-10 06:27:54,300] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0451 (0.1546)	gLvNorm 0.0242 (0.1388)	mvpNorm 0.0976 (0.2931)

[2022-08-10 06:28:53,060] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0723 (0.1541)	gLvNorm 0.3108 (0.1384)	mvpNorm 0.4189 (0.2918)

[2022-08-10 06:29:51,926] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0579 (0.1537)	gLvNorm 0.0427 (0.1389)	mvpNorm 0.0220 (0.2934)

[2022-08-10 06:30:50,775] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0076 (0.1536)	gLvNorm 0.0306 (0.1396)	mvpNorm 0.0106 (0.2947)

[2022-08-10 06:31:49,526] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.2078 (0.1522)	gLvNorm 0.0180 (0.1392)	mvpNorm 0.1400 (0.2914)

[2022-08-10 06:32:48,315] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0500 (0.1528)	gLvNorm 0.0478 (0.1386)	mvpNorm 0.0154 (0.2895)

[2022-08-10 06:33:47,051] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0298 (0.1527)	gLvNorm 0.0380 (0.1374)	mvpNorm 0.0166 (0.2871)

[2022-08-10 06:34:45,781] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0328 (0.1523)	gLvNorm 0.0356 (0.1395)	mvpNorm 0.0655 (0.2897)

[2022-08-10 06:35:44,457] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.022414	gLtNorm 0.0202 (0.1509)	gLvNorm 0.0520 (0.1394)	mvpNorm 0.0136 (0.2881)

[2022-08-10 06:52:49,167] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 13	 Inner Train loss: 0.5177, acc=0.8113, lr=0.000010	
[2022-08-10 06:54:05,204] automodels.py->Med_innerTest line:856 [INFO]Epoch: 13	 Test loss: 1.0450, score: 0.6625
[2022-08-10 06:54:05,204] implicit-augment.py->main line:455 [INFO]SAVING trained model at epoch 13 with 0.6625 Dice score
[2022-08-10 06:54:05,615] implicit-augment.py->main line:429 [INFO]70% (14/20)
[2022-08-10 06:54:06,156] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.4042 (0.4042)	gLvNorm 0.0195 (0.0195)	mvpNorm 0.5829 (0.5829)

[2022-08-10 06:55:05,024] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0599 (0.1261)	gLvNorm 0.2595 (0.1403)	mvpNorm 0.5116 (0.2813)

[2022-08-10 06:56:03,881] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0479 (0.1438)	gLvNorm 0.0187 (0.1322)	mvpNorm 0.0505 (0.2814)

[2022-08-10 06:57:02,723] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0236 (0.1424)	gLvNorm 0.0444 (0.1332)	mvpNorm 0.0147 (0.2666)

[2022-08-10 06:58:01,626] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0293 (0.1393)	gLvNorm 0.1003 (0.1248)	mvpNorm 0.0426 (0.2521)

[2022-08-10 06:59:00,519] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.3253 (0.1445)	gLvNorm 0.0704 (0.1257)	mvpNorm 0.1029 (0.2615)

[2022-08-10 06:59:59,535] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0417 (0.1469)	gLvNorm 0.1951 (0.1257)	mvpNorm 0.1681 (0.2661)

[2022-08-10 07:00:58,315] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0439 (0.1494)	gLvNorm 0.0217 (0.1280)	mvpNorm 0.0063 (0.2746)

[2022-08-10 07:01:57,213] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.2861 (0.1425)	gLvNorm 0.3426 (0.1258)	mvpNorm 1.2397 (0.2668)

[2022-08-10 07:02:56,038] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0304 (0.1381)	gLvNorm 0.0473 (0.1274)	mvpNorm 0.0100 (0.2653)

[2022-08-10 07:03:54,912] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0965 (0.1422)	gLvNorm 0.0677 (0.1283)	mvpNorm 0.1653 (0.2683)

[2022-08-10 07:04:53,831] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 2.3822 (0.1386)	gLvNorm 0.3863 (0.1283)	mvpNorm 1.2744 (0.2634)

[2022-08-10 07:05:52,961] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.3018 (0.1358)	gLvNorm 0.0667 (0.1260)	mvpNorm 0.1378 (0.2586)

[2022-08-10 07:06:51,970] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0485 (0.1379)	gLvNorm 0.0302 (0.1260)	mvpNorm 0.0896 (0.2614)

[2022-08-10 07:07:50,947] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0563 (0.1398)	gLvNorm 0.1265 (0.1262)	mvpNorm 0.0676 (0.2634)

[2022-08-10 07:08:49,900] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0462 (0.1369)	gLvNorm 0.0997 (0.1258)	mvpNorm 0.2398 (0.2606)

[2022-08-10 07:09:48,785] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0619 (0.1400)	gLvNorm 0.2275 (0.1254)	mvpNorm 0.4078 (0.2628)

[2022-08-10 07:10:47,661] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.1465 (0.1382)	gLvNorm 0.1089 (0.1268)	mvpNorm 0.4945 (0.2640)

[2022-08-10 07:11:46,779] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0880 (0.1373)	gLvNorm 0.0224 (0.1256)	mvpNorm 0.1031 (0.2603)

[2022-08-10 07:12:45,638] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.4626 (0.1367)	gLvNorm 0.0702 (0.1269)	mvpNorm 0.8507 (0.2596)

[2022-08-10 07:13:44,528] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0995 (0.1363)	gLvNorm 0.7955 (0.1257)	mvpNorm 0.4561 (0.2579)

[2022-08-10 07:14:43,438] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.1636 (0.1355)	gLvNorm 0.0995 (0.1252)	mvpNorm 0.0350 (0.2560)

[2022-08-10 07:15:42,388] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0911 (0.1358)	gLvNorm 0.1474 (0.1245)	mvpNorm 0.3029 (0.2559)

[2022-08-10 07:16:41,359] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.017307	gLtNorm 0.0254 (0.1361)	gLvNorm 0.0428 (0.1251)	mvpNorm 0.1156 (0.2565)

[2022-08-10 07:33:44,817] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 14	 Inner Train loss: 0.5159, acc=0.8124, lr=0.000010	
[2022-08-10 07:35:00,740] automodels.py->Med_innerTest line:856 [INFO]Epoch: 14	 Test loss: 1.0734, score: 0.6590
[2022-08-10 07:35:00,741] implicit-augment.py->main line:429 [INFO]75% (15/20)
[2022-08-10 07:35:01,282] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0257 (0.0257)	gLvNorm 0.1104 (0.1104)	mvpNorm 0.1657 (0.1657)

[2022-08-10 07:36:00,022] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0148 (0.1279)	gLvNorm 0.1640 (0.1130)	mvpNorm 0.1998 (0.2331)

[2022-08-10 07:36:58,688] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.2578 (0.1111)	gLvNorm 0.0456 (0.1279)	mvpNorm 0.2965 (0.2184)

[2022-08-10 07:37:57,411] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0359 (0.1208)	gLvNorm 0.0604 (0.1218)	mvpNorm 0.1421 (0.2248)

[2022-08-10 07:38:56,158] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.1866 (0.1252)	gLvNorm 0.1044 (0.1276)	mvpNorm 0.2014 (0.2423)

[2022-08-10 07:39:54,973] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0136 (0.1254)	gLvNorm 0.0528 (0.1293)	mvpNorm 0.0233 (0.2452)

[2022-08-10 07:40:53,898] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0212 (0.1282)	gLvNorm 0.0596 (0.1264)	mvpNorm 0.1070 (0.2473)

[2022-08-10 07:41:52,679] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.2025 (0.1311)	gLvNorm 0.0046 (0.1254)	mvpNorm 0.1943 (0.2507)

[2022-08-10 07:42:51,499] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0310 (0.1292)	gLvNorm 0.0387 (0.1244)	mvpNorm 0.0187 (0.2475)

[2022-08-10 07:43:50,262] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.1628 (0.1296)	gLvNorm 0.0464 (0.1280)	mvpNorm 0.2095 (0.2501)

[2022-08-10 07:44:48,998] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0175 (0.1312)	gLvNorm 0.0271 (0.1272)	mvpNorm 0.0039 (0.2512)

[2022-08-10 07:45:47,702] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.1053 (0.1327)	gLvNorm 0.1828 (0.1268)	mvpNorm 0.4325 (0.2529)

[2022-08-10 07:46:46,608] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.2532 (0.1304)	gLvNorm 0.0309 (0.1265)	mvpNorm 0.3548 (0.2506)

[2022-08-10 07:47:45,276] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0617 (0.1305)	gLvNorm 0.0541 (0.1283)	mvpNorm 0.0015 (0.2513)

[2022-08-10 07:48:44,133] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0601 (0.1323)	gLvNorm 0.0237 (0.1286)	mvpNorm 0.0842 (0.2548)

[2022-08-10 07:49:43,055] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0157 (0.1334)	gLvNorm 0.0219 (0.1264)	mvpNorm 0.0386 (0.2535)

[2022-08-10 07:50:41,767] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0754 (0.1351)	gLvNorm 0.0332 (0.1266)	mvpNorm 0.0169 (0.2568)

[2022-08-10 07:51:40,510] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0028 (0.1344)	gLvNorm 0.0981 (0.1276)	mvpNorm 0.1249 (0.2566)

[2022-08-10 07:52:39,347] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.3764 (0.1352)	gLvNorm 0.2639 (0.1263)	mvpNorm 0.0185 (0.2553)

[2022-08-10 07:53:38,035] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0672 (0.1360)	gLvNorm 0.0258 (0.1267)	mvpNorm 0.0649 (0.2566)

[2022-08-10 07:54:36,860] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.3571 (0.1354)	gLvNorm 0.2011 (0.1268)	mvpNorm 0.9117 (0.2565)

[2022-08-10 07:55:35,797] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.0442 (0.1349)	gLvNorm 0.1220 (0.1277)	mvpNorm 0.0790 (0.2571)

[2022-08-10 07:56:34,736] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.1042 (0.1347)	gLvNorm 0.2544 (0.1271)	mvpNorm 0.5756 (0.2552)

[2022-08-10 07:57:33,520] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.012538	gLtNorm 0.1891 (0.1334)	gLvNorm 0.0988 (0.1269)	mvpNorm 0.4571 (0.2520)

[2022-08-10 08:14:38,304] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 15	 Inner Train loss: 0.5112, acc=0.8137, lr=0.000010	
[2022-08-10 08:15:54,531] automodels.py->Med_innerTest line:856 [INFO]Epoch: 15	 Test loss: 1.1628, score: 0.6403
[2022-08-10 08:15:54,531] implicit-augment.py->main line:429 [INFO]80% (16/20)
[2022-08-10 08:15:55,082] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 4.7778 (4.7778)	gLvNorm 0.0192 (0.0192)	mvpNorm 5.2987 (5.2987)

[2022-08-10 08:16:54,088] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0058 (0.1826)	gLvNorm 0.7261 (0.1747)	mvpNorm 0.7517 (0.3304)

[2022-08-10 08:17:53,002] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0888 (0.1860)	gLvNorm 0.0589 (0.1728)	mvpNorm 0.0203 (0.3438)

[2022-08-10 08:18:51,929] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0457 (0.1729)	gLvNorm 0.0276 (0.1580)	mvpNorm 0.1303 (0.3110)

[2022-08-10 08:19:50,792] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0250 (0.1662)	gLvNorm 0.2384 (0.1528)	mvpNorm 0.3901 (0.3002)

[2022-08-10 08:20:49,707] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0112 (0.1696)	gLvNorm 0.0076 (0.1562)	mvpNorm 0.0029 (0.3070)

[2022-08-10 08:21:48,811] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0526 (0.1675)	gLvNorm 0.0452 (0.1600)	mvpNorm 0.0216 (0.3068)

[2022-08-10 08:22:47,654] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0338 (0.1650)	gLvNorm 0.2527 (0.1627)	mvpNorm 0.2648 (0.3091)

[2022-08-10 08:23:46,611] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.1867 (0.1635)	gLvNorm 0.1151 (0.1647)	mvpNorm 0.0273 (0.3095)

[2022-08-10 08:24:45,548] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0317 (0.1629)	gLvNorm 0.0027 (0.1625)	mvpNorm 0.0291 (0.3096)

[2022-08-10 08:25:44,500] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0133 (0.1570)	gLvNorm 0.0203 (0.1627)	mvpNorm 0.0016 (0.3060)

[2022-08-10 08:26:43,392] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0404 (0.1577)	gLvNorm 0.0466 (0.1624)	mvpNorm 0.1181 (0.3044)

[2022-08-10 08:27:42,508] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.1098 (0.1554)	gLvNorm 0.0967 (0.1594)	mvpNorm 0.0467 (0.3000)

[2022-08-10 08:28:41,374] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.2150 (0.1557)	gLvNorm 0.0251 (0.1625)	mvpNorm 0.3821 (0.3031)

[2022-08-10 08:29:40,187] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0233 (0.1546)	gLvNorm 0.1612 (0.1683)	mvpNorm 0.1048 (0.3083)

[2022-08-10 08:30:38,912] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0298 (0.1555)	gLvNorm 0.0221 (0.1647)	mvpNorm 0.0385 (0.3054)

[2022-08-10 08:31:37,680] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0180 (0.1596)	gLvNorm 0.0230 (0.1644)	mvpNorm 0.0407 (0.3077)

[2022-08-10 08:32:36,534] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 2.8191 (0.1598)	gLvNorm 0.1648 (0.1623)	mvpNorm 4.2894 (0.3070)

[2022-08-10 08:33:35,522] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0378 (0.1586)	gLvNorm 0.8024 (0.1605)	mvpNorm 0.8453 (0.3032)

[2022-08-10 08:34:34,287] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0349 (0.1600)	gLvNorm 0.0534 (0.1610)	mvpNorm 0.0093 (0.3055)

[2022-08-10 08:35:32,978] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0394 (0.1629)	gLvNorm 0.0570 (0.1610)	mvpNorm 0.0326 (0.3107)

[2022-08-10 08:36:31,763] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0337 (0.1617)	gLvNorm 0.4117 (0.1599)	mvpNorm 0.5133 (0.3083)

[2022-08-10 08:37:30,528] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.1072 (0.1620)	gLvNorm 0.0272 (0.1601)	mvpNorm 0.1183 (0.3094)

[2022-08-10 08:38:29,255] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.008313	gLtNorm 0.0341 (0.1614)	gLvNorm 0.0822 (0.1604)	mvpNorm 0.0503 (0.3101)

[2022-08-10 08:55:33,775] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 16	 Inner Train loss: 0.5076, acc=0.8151, lr=0.000010	
[2022-08-10 08:56:49,632] automodels.py->Med_innerTest line:856 [INFO]Epoch: 16	 Test loss: 1.1449, score: 0.6612
[2022-08-10 08:56:49,632] implicit-augment.py->main line:429 [INFO]85% (17/20)
[2022-08-10 08:56:50,184] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.4760 (0.4760)	gLvNorm 2.4458 (2.4458)	mvpNorm 0.8518 (0.8518)

[2022-08-10 08:57:48,871] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0347 (0.3459)	gLvNorm 0.0130 (0.3159)	mvpNorm 0.0108 (0.4870)

[2022-08-10 08:58:47,496] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0545 (0.3708)	gLvNorm 0.1056 (0.3115)	mvpNorm 0.0183 (0.5256)

[2022-08-10 08:59:46,223] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0250 (0.4046)	gLvNorm 0.0191 (0.3341)	mvpNorm 0.0125 (0.5718)

[2022-08-10 09:00:45,006] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.1077 (0.3926)	gLvNorm 0.0471 (0.3484)	mvpNorm 0.1081 (0.5673)

[2022-08-10 09:01:43,812] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.1483 (0.3744)	gLvNorm 0.0593 (0.3359)	mvpNorm 0.3652 (0.5482)

[2022-08-10 09:02:42,795] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0057 (0.3851)	gLvNorm 0.0575 (0.3302)	mvpNorm 0.0528 (0.5578)

[2022-08-10 09:03:41,601] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0521 (0.3700)	gLvNorm 0.0194 (0.3289)	mvpNorm 0.0898 (0.5506)

[2022-08-10 09:04:40,323] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0163 (0.3772)	gLvNorm 0.5129 (0.3267)	mvpNorm 0.5745 (0.5456)

[2022-08-10 09:05:39,078] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.3706 (0.3844)	gLvNorm 0.9431 (0.3306)	mvpNorm 0.2802 (0.5528)

[2022-08-10 09:06:37,886] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0302 (0.3794)	gLvNorm 0.4829 (0.3315)	mvpNorm 0.6328 (0.5500)

[2022-08-10 09:07:36,646] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0039 (0.3717)	gLvNorm 0.2465 (0.3291)	mvpNorm 0.2590 (0.5379)

[2022-08-10 09:08:35,576] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0640 (0.3623)	gLvNorm 0.0098 (0.3273)	mvpNorm 0.0827 (0.5285)

[2022-08-10 09:09:34,370] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0506 (0.3543)	gLvNorm 0.3761 (0.3326)	mvpNorm 0.1587 (0.5235)

[2022-08-10 09:10:33,179] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.3291 (0.3468)	gLvNorm 0.0819 (0.3362)	mvpNorm 0.2735 (0.5189)

[2022-08-10 09:11:31,950] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0353 (0.3418)	gLvNorm 0.0266 (0.3350)	mvpNorm 0.0157 (0.5103)

[2022-08-10 09:12:30,723] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0056 (0.3421)	gLvNorm 0.0133 (0.3356)	mvpNorm 0.0159 (0.5069)

[2022-08-10 09:13:29,513] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0935 (0.3374)	gLvNorm 0.0209 (0.3287)	mvpNorm 0.0327 (0.4986)

[2022-08-10 09:14:28,474] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0404 (0.3354)	gLvNorm 0.3087 (0.3292)	mvpNorm 0.5019 (0.4933)

[2022-08-10 09:15:27,233] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0718 (0.3367)	gLvNorm 0.4333 (0.3331)	mvpNorm 0.2967 (0.4994)

[2022-08-10 09:16:25,931] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.2296 (0.3330)	gLvNorm 0.0151 (0.3289)	mvpNorm 0.1933 (0.4939)

[2022-08-10 09:17:24,786] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.1242 (0.3339)	gLvNorm 0.0444 (0.3294)	mvpNorm 0.0251 (0.4959)

[2022-08-10 09:18:23,514] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.1148 (0.3348)	gLvNorm 0.0135 (0.3271)	mvpNorm 0.0751 (0.4925)

[2022-08-10 09:19:22,286] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.004820	gLtNorm 0.0067 (0.3326)	gLvNorm 0.1801 (0.3261)	mvpNorm 0.1950 (0.4889)

[2022-08-10 09:36:26,624] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 17	 Inner Train loss: 0.5042, acc=0.8164, lr=0.000010	
[2022-08-10 09:37:42,754] automodels.py->Med_innerTest line:856 [INFO]Epoch: 17	 Test loss: 1.0745, score: 0.6611
[2022-08-10 09:37:42,755] implicit-augment.py->main line:429 [INFO]90% (18/20)
[2022-08-10 09:37:43,285] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.7453 (0.7453)	gLvNorm 0.0661 (0.0661)	mvpNorm 0.4731 (0.4731)

[2022-08-10 09:38:42,235] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.7210 (0.1475)	gLvNorm 0.0554 (0.2308)	mvpNorm 1.1738 (0.3179)

[2022-08-10 09:39:40,966] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0282 (0.1583)	gLvNorm 0.0512 (0.1911)	mvpNorm 0.0852 (0.3036)

[2022-08-10 09:40:39,767] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0229 (0.1466)	gLvNorm 0.0430 (0.1696)	mvpNorm 0.0151 (0.2839)

[2022-08-10 09:41:38,584] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.4352 (0.1542)	gLvNorm 0.0157 (0.1643)	mvpNorm 0.3640 (0.2812)

[2022-08-10 09:42:37,344] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.1365 (0.1489)	gLvNorm 0.0508 (0.1622)	mvpNorm 0.2603 (0.2760)

[2022-08-10 09:43:36,351] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0141 (0.1499)	gLvNorm 0.0217 (0.1649)	mvpNorm 0.0018 (0.2842)

[2022-08-10 09:44:35,262] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0623 (0.1395)	gLvNorm 0.3897 (0.1683)	mvpNorm 0.4269 (0.2777)

[2022-08-10 09:45:34,158] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.2744 (0.1453)	gLvNorm 0.0781 (0.1687)	mvpNorm 0.5337 (0.2799)

[2022-08-10 09:46:33,005] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0344 (0.1477)	gLvNorm 0.0468 (0.1736)	mvpNorm 0.0037 (0.2857)

[2022-08-10 09:47:31,875] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.1008 (0.1503)	gLvNorm 0.0844 (0.1693)	mvpNorm 0.3502 (0.2844)

[2022-08-10 09:48:30,684] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.1461 (0.1480)	gLvNorm 0.0033 (0.1668)	mvpNorm 0.1729 (0.2807)

[2022-08-10 09:49:29,587] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0120 (0.1505)	gLvNorm 0.5547 (0.1660)	mvpNorm 0.6002 (0.2841)

[2022-08-10 09:50:28,325] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0508 (0.1493)	gLvNorm 0.4298 (0.1649)	mvpNorm 0.6227 (0.2835)

[2022-08-10 09:51:26,998] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0320 (0.1532)	gLvNorm 0.0132 (0.1645)	mvpNorm 0.0150 (0.2857)

[2022-08-10 09:52:25,623] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0169 (0.1541)	gLvNorm 0.0899 (0.1670)	mvpNorm 0.0628 (0.2903)

[2022-08-10 09:53:24,359] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0546 (0.1540)	gLvNorm 0.4469 (0.1659)	mvpNorm 0.3226 (0.2899)

[2022-08-10 09:54:23,075] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0147 (0.1544)	gLvNorm 0.0230 (0.1681)	mvpNorm 0.0138 (0.2953)

[2022-08-10 09:55:21,976] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.1865 (0.1547)	gLvNorm 0.0395 (0.1670)	mvpNorm 0.3362 (0.2942)

[2022-08-10 09:56:20,814] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0336 (0.1575)	gLvNorm 0.3407 (0.1695)	mvpNorm 0.5232 (0.2997)

[2022-08-10 09:57:19,498] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.3265 (0.1583)	gLvNorm 0.0023 (0.1676)	mvpNorm 0.3476 (0.2990)

[2022-08-10 09:58:18,227] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 1.4812 (0.1574)	gLvNorm 0.0720 (0.1664)	mvpNorm 1.0316 (0.2974)

[2022-08-10 09:59:16,990] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0487 (0.1609)	gLvNorm 0.0152 (0.1690)	mvpNorm 0.0196 (0.3016)

[2022-08-10 10:00:15,679] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.002209	gLtNorm 0.0565 (0.1633)	gLvNorm 1.2193 (0.1670)	mvpNorm 1.2140 (0.3021)

[2022-08-10 10:17:20,231] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 18	 Inner Train loss: 0.5001, acc=0.8180, lr=0.000010	
[2022-08-10 10:18:36,094] automodels.py->Med_innerTest line:856 [INFO]Epoch: 18	 Test loss: 1.0705, score: 0.6581
[2022-08-10 10:18:36,095] implicit-augment.py->main line:429 [INFO]95% (19/20)
[2022-08-10 10:18:36,625] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 0% (0/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0772 (0.0772)	gLvNorm 0.1466 (0.1466)	mvpNorm 0.0323 (0.0323)

[2022-08-10 10:19:35,528] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 4% (200/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0426 (0.1145)	gLvNorm 0.1044 (0.1126)	mvpNorm 0.1555 (0.2269)

[2022-08-10 10:20:34,407] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 8% (400/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0535 (0.1222)	gLvNorm 0.0467 (0.1141)	mvpNorm 0.1364 (0.2272)

[2022-08-10 10:21:33,144] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 13% (600/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0556 (0.1217)	gLvNorm 0.0357 (0.1182)	mvpNorm 0.0277 (0.2353)

[2022-08-10 10:22:31,951] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 17% (800/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0340 (0.1167)	gLvNorm 0.1561 (0.1187)	mvpNorm 0.1863 (0.2331)

[2022-08-10 10:23:30,838] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 21% (1000/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.1717 (0.1156)	gLvNorm 0.0166 (0.1182)	mvpNorm 0.2066 (0.2352)

[2022-08-10 10:24:29,843] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 25% (1200/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0697 (0.1198)	gLvNorm 0.0724 (0.1184)	mvpNorm 0.0841 (0.2367)

[2022-08-10 10:25:28,639] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 29% (1400/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.2598 (0.1199)	gLvNorm 0.3118 (0.1174)	mvpNorm 0.0234 (0.2302)

[2022-08-10 10:26:27,462] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 34% (1600/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0217 (0.1213)	gLvNorm 0.0546 (0.1187)	mvpNorm 0.0553 (0.2348)

[2022-08-10 10:27:26,206] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 38% (1800/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0388 (0.1218)	gLvNorm 0.0847 (0.1153)	mvpNorm 0.0189 (0.2314)

[2022-08-10 10:28:24,937] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 42% (2000/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0171 (0.1228)	gLvNorm 0.0237 (0.1178)	mvpNorm 0.0267 (0.2338)

[2022-08-10 10:29:23,777] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 46% (2200/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0729 (0.1196)	gLvNorm 0.0121 (0.1170)	mvpNorm 0.1403 (0.2307)

[2022-08-10 10:30:22,778] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 50% (2400/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.1766 (0.1203)	gLvNorm 0.0700 (0.1176)	mvpNorm 0.0935 (0.2320)

[2022-08-10 10:31:21,700] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 55% (2600/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0224 (0.1264)	gLvNorm 0.0119 (0.1204)	mvpNorm 0.0100 (0.2412)

[2022-08-10 10:32:20,564] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 59% (2800/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0686 (0.1243)	gLvNorm 0.0477 (0.1197)	mvpNorm 0.0057 (0.2371)

[2022-08-10 10:33:19,335] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 63% (3000/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.2102 (0.1239)	gLvNorm 0.0213 (0.1191)	mvpNorm 0.1548 (0.2365)

[2022-08-10 10:34:18,157] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 67% (3200/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.3577 (0.1236)	gLvNorm 0.2843 (0.1179)	mvpNorm 1.0454 (0.2336)

[2022-08-10 10:35:16,927] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 71% (3400/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.1573 (0.1252)	gLvNorm 0.0718 (0.1180)	mvpNorm 0.0205 (0.2355)

[2022-08-10 10:36:15,900] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 76% (3600/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0331 (0.1240)	gLvNorm 0.4302 (0.1170)	mvpNorm 0.5688 (0.2347)

[2022-08-10 10:37:14,679] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 80% (3800/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0156 (0.1222)	gLvNorm 0.0236 (0.1170)	mvpNorm 0.0229 (0.2331)

[2022-08-10 10:38:13,516] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 84% (4000/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0319 (0.1224)	gLvNorm 0.0389 (0.1173)	mvpNorm 0.0347 (0.2340)

[2022-08-10 10:39:12,421] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 88% (4200/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.2277 (0.1230)	gLvNorm 0.0826 (0.1175)	mvpNorm 0.2404 (0.2353)

[2022-08-10 10:40:11,406] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 92% (4400/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.1126 (0.1230)	gLvNorm 0.0110 (0.1180)	mvpNorm 0.0635 (0.2351)

[2022-08-10 10:41:10,338] automodels.py->Med_hyperHesTrain line:635 [INFO]hyperTrain batch 97% (4600/4766), task_lr=0.000010, hyper_lr=0.000596	gLtNorm 0.0366 (0.1263)	gLvNorm 0.0713 (0.1173)	mvpNorm 0.0202 (0.2381)

[2022-08-10 10:58:15,351] automodels.py->Med_innerTrain line:694 [INFO]Epoch: 19	 Inner Train loss: 0.4950, acc=0.8200, lr=0.000010	
[2022-08-10 10:59:31,439] automodels.py->Med_innerTest line:856 [INFO]Epoch: 19	 Test loss: 1.1129, score: 0.6530
[2022-08-10 10:59:31,607] implicit-augment.py->main line:477 [INFO]save train history at: /home/rayeh/workspace/project/med/Med_AutoDO/picture/UNet_opt_HES_est_True_aug_model_SEP_los_model_NONE_ir_1_sr_1.0_nr_0.0.jpg
[2022-08-10 10:59:31,607] implicit-augment.py->main line:479 [INFO]BEST trained model has 0.6625 Dice score
